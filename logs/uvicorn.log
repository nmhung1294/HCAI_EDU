[Tue May 13 10:12:30 AM +07 2025] Starting Uvicorn...
INFO:     Will watch for changes in these directories: ['/home/ldmt/Projects/HCAI_Edu']
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [143727] using WatchFiles
INFO:     Started server process [143735]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
WARNING:  WatchFiles detected changes in 'models/dictionary_query.py'. Reloading...
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [143735]
INFO:     Started server process [144477]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
Tables in the database: dict_keys(['PJS_points', 'users'])
INFO:     127.0.0.1:44280 - "OPTIONS /chat HTTP/1.1" 200 OK
Generated query: ["Cumulative"]

<class 'llama_index.core.base.llms.types.CompletionResponse'>
Traceback (most recent call last):
  File "/home/ldmt/Projects/HCAI_Edu/models/dictionary_query.py", line 52, in parse_word_list
    return ast.literal_eval(output_str)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/ast.py", line 110, in literal_eval
    return _convert(node_or_string)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/ast.py", line 109, in _convert
    return _convert_signed_num(node)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/ast.py", line 83, in _convert_signed_num
    return _convert_num(node)
           ^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/ast.py", line 74, in _convert_num
    _raise_malformed_node(node)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/ast.py", line 71, in _raise_malformed_node
    raise ValueError(msg + f': {node!r}')
ValueError: malformed node or string: CompletionResponse(text='["Cumulative"]\n', additional_kwargs={}, raw={'content': {'parts': [{'video_metadata': None, 'thought': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': '["Cumulative"]\n'}], 'role': 'model'}, 'citation_metadata': None, 'finish_message': None, 'token_count': None, 'finish_reason': <FinishReason.STOP: 'STOP'>, 'avg_logprobs': -0.001327107078395784, 'grounding_metadata': None, 'index': None, 'logprobs_result': None, 'safety_ratings': None, 'usage_metadata': {'cache_tokens_details': None, 'cached_content_token_count': None, 'candidates_token_count': 4, 'candidates_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>, 'token_count': 4}], 'prompt_token_count': 174, 'prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>, 'token_count': 174}], 'thoughts_token_count': None, 'tool_use_prompt_token_count': None, 'tool_use_prompt_tokens_details': None, 'total_token_count': 178, 'traffic_type': None}}, logprobs=None, delta=None)

Word list: []
Result: 
DICTIONARY INTENT
None
INFO:     127.0.0.1:44294 - "POST /chat HTTP/1.1" 200 OK
WARNING:  WatchFiles detected changes in 'models/dictionary_query.py'. Reloading...
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [144477]
INFO:     Started server process [145954]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
Tables in the database: dict_keys(['PJS_points', 'users'])
Generated query: ["Cumulative"]

<class 'llama_index.core.base.llms.types.CompletionResponse'>
INFO:     127.0.0.1:58540 - "POST /chat HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py", line 409, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/fastapi/applications.py", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/applications.py", line 112, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/middleware/errors.py", line 187, in __call__
    raise exc
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/middleware/errors.py", line 165, in __call__
    await self.app(scope, receive, _send)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/middleware/cors.py", line 93, in __call__
    await self.simple_response(scope, receive, send, request_headers=headers)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/middleware/cors.py", line 144, in simple_response
    await self.app(scope, receive, send)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/middleware/exceptions.py", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/routing.py", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/routing.py", line 735, in app
    await route.handle(scope, receive, send)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/routing.py", line 288, in handle
    await self.app(scope, receive, send)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/routing.py", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/routing.py", line 73, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/fastapi/routing.py", line 301, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/fastapi/routing.py", line 214, in run_endpoint_function
    return await run_in_threadpool(dependant.call, **values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/starlette/concurrency.py", line 37, in run_in_threadpool
    return await anyio.to_thread.run_sync(func)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/anyio/to_thread.py", line 56, in run_sync
    return await get_async_backend().run_sync_in_worker_thread(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/anyio/_backends/_asyncio.py", line 2470, in run_sync_in_worker_thread
    return await future
           ^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/anyio/_backends/_asyncio.py", line 967, in run
    result = context.run(func, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ldmt/Projects/HCAI_Edu/app.py", line 37, in chat_with_bot
    bot_response = get_chatbot_response(f"User: {user_input}\nBot:")
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ldmt/Projects/HCAI_Edu/models/chat.py", line 111, in get_chatbot_response
    response = router_query_engine.query(user_prompt)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 322, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py", line 52, in query
    query_result = self._query(str_or_query_bundle)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 322, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/llama_index/core/query_engine/router_query_engine.py", line 194, in _query
    final_response = selected_query_engine.query(query_bundle)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 322, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/llama_index/core/query_engine/custom.py", line 43, in query
    raw_response = self.custom_query(query_str)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py", line 322, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ldmt/Projects/HCAI_Edu/models/dictionary_query.py", line 71, in custom_query
    word_list = parse_word_list(generated_query)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ldmt/Projects/HCAI_Edu/models/dictionary_query.py", line 52, in parse_word_list
    return ast.literal_eval(output_str.strip())
                            ^^^^^^^^^^^^^^^^
  File "/media/ldmt/Data/Projects/HCAI_Edu/conda_env/lib/python3.11/site-packages/pydantic/main.py", line 994, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
AttributeError: 'CompletionResponse' object has no attribute 'strip'
WARNING:  WatchFiles detected changes in 'models/dictionary_query.py'. Reloading...
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [145954]
[Tue May 13 10:17:55 AM +07 2025] Script interrupted. Shutting down.
[Tue May 13 10:17:56 AM +07 2025] Starting Uvicorn...
INFO:     Will watch for changes in these directories: ['/home/ldmt/Projects/HCAI_Edu']
ERROR:    [Errno 98] Address already in use
[Tue May 13 10:17:56 AM +07 2025] Uvicorn stopped.
[Tue May 13 10:18:01 AM +07 2025] Script interrupted. Shutting down.
[Tue May 13 10:18:02 AM +07 2025] Starting Uvicorn...
INFO:     Will watch for changes in these directories: ['/home/ldmt/Projects/HCAI_Edu']
ERROR:    [Errno 98] Address already in use
[Tue May 13 10:18:03 AM +07 2025] Uvicorn stopped.
INFO:     Started server process [147299]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Stopping reloader process [143727]
[Tue May 13 10:18:18 AM +07 2025] Uvicorn stopped.
[Tue May 13 10:18:27 AM +07 2025] Script interrupted. Shutting down.
[Tue May 13 10:18:56 AM +07 2025] Starting Uvicorn...
INFO:     Will watch for changes in these directories: ['/home/ldmt/Projects/HCAI_Edu']
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [148202] using WatchFiles
INFO:     Started server process [148210]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
Tables in the database: dict_keys(['PJS_points', 'users'])
Generated query: ["Cumulative"]

Word list: ['Cumulative']
Result: Word: cumulative | Definitions: Incorporating all current and previous data up to the present or at the time of measuring or collating; That is formed by an accumulation of successive additions; That tends to accumulate; Having priority rights to receive a dividend that accrue until paid | Synonyms: None | Examples: None

DICTIONARY INTENT
Word: cumulative | Definitions: Incorporating all current and previous data up to the present or at the time of measuring or collating; That is formed by an accumulation of successive additions; That tends to accumulate; Having priority rights to receive a dividend that accrue until paid | Synonyms: None | Examples: None

INFO:     127.0.0.1:46702 - "POST /chat HTTP/1.1" 200 OK
Generated query: ["cumulative"]

Word list: ['cumulative']
Result: Word: cumulative | Definitions: Incorporating all current and previous data up to the present or at the time of measuring or collating; That is formed by an accumulation of successive additions; That tends to accumulate; Having priority rights to receive a dividend that accrue until paid | Synonyms: None | Examples: None

DICTIONARY INTENT
Word: cumulative | Definitions: Incorporating all current and previous data up to the present or at the time of measuring or collating; That is formed by an accumulation of successive additions; That tends to accumulate; Having priority rights to receive a dividend that accrue until paid | Synonyms: None | Examples: None

INFO:     127.0.0.1:45750 - "POST /chat HTTP/1.1" 200 OK
[Tue May 13 10:20:21 AM +07 2025] Script interrupted. Shutting down.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [148210]
INFO:     Stopping reloader process [148202]
[Tue May 13 10:20:21 AM +07 2025] Uvicorn stopped.
[Tue May 13 05:08:24 PM +07 2025] Starting Uvicorn...
INFO:     Will watch for changes in these directories: ['/media/ldmt/Data/Projects/HCAI_Edu']
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [30780] using WatchFiles
INFO:httpx:HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash "HTTP/1.1 200 OK"
INFO:     Started server process [30789]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
Tables in the database: dict_keys(['PJS_points', 'users'])
INFO:     127.0.0.1:55520 - "OPTIONS /delete_pdf/ HTTP/1.1" 200 OK
INFO:     127.0.0.1:55520 - "DELETE /delete_pdf/ HTTP/1.1" 200 OK
INFO:httpx:HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash "HTTP/1.1 200 OK"
INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
Initializing RAPTOR with collection_name: %s IuLAyPWiZwV4unB2TdXqvXpyXup1
Force rebuilding collection...
Collection 'IuLAyPWiZwV4unB2TdXqvXpyXup1' does not exist. Skipping deletion.
Loading provided documents...
Creating RaptorPack and building raptor tree...
Generating embeddings for level 0.
Performing clustering for level 0.
Generating summaries for level 0 with 5 clusters.
Level 0 created summaries/clusters: 5
Generating embeddings for level 1.
Performing clustering for level 1.
Generating summaries for level 1 with 1 clusters.
Level 1 created summaries/clusters: 1
Generating embeddings for level 2.
Performing clustering for level 2.
Generating summaries for level 2 with 1 clusters.
Level 2 created summaries/clusters: 1
Setting up RaptorRetriever
Setting up RetrieverQueryEngine
INFO:     127.0.0.1:47846 - "POST /upload_pdf/ HTTP/1.1" 200 OK
INFO:     127.0.0.1:55342 - "OPTIONS /chat_with_file HTTP/1.1" 200 OK
INFO:httpx:HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash "HTTP/1.1 200 OK"
ERROR:asyncio:Task exception was never retrieved
future: <Task finished name='Task-28' coro=<AsyncClient.aclose() done, defined at /media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/httpx/_client.py:1978> exception=RuntimeError('unable to perform operation on <TCPTransport closed=True reading=False 0x7f24bf3a3f90>; the handler is closed')>
Traceback (most recent call last):
  File "/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/httpx/_client.py", line 1985, in aclose
    await self._transport.aclose()
  File "/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/httpx/_transports/default.py", line 406, in aclose
    await self._pool.aclose()
  File "/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 353, in aclose
    await self._close_connections(closing_connections)
  File "/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 345, in _close_connections
    await connection.aclose()
  File "/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/httpcore/_async/connection.py", line 173, in aclose
    await self._connection.aclose()
  File "/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 258, in aclose
    await self._network_stream.aclose()
  File "/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/httpcore/_backends/anyio.py", line 53, in aclose
    await self._stream.aclose()
  File "/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/anyio/streams/tls.py", line 216, in aclose
    await self.transport_stream.aclose()
  File "/media/ldmt/Data/Projects/HCAI_Edu/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py", line 1310, in aclose
    self._transport.write_eof()
  File "uvloop/handles/stream.pyx", line 703, in uvloop.loop.UVStream.write_eof
  File "uvloop/handles/handle.pyx", line 159, in uvloop.loop.UVHandle._ensure_alive
RuntimeError: unable to perform operation on <TCPTransport closed=True reading=False 0x7f24bf3a3f90>; the handler is closed
INFO:httpx:HTTP Request: POST https://api.cohere.com/v1/embed "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:google_genai.models:AFC remote call 1 is done.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC remote call 1 is done.
Initializing RAPTOR with collection_name: %s IuLAyPWiZwV4unB2TdXqvXpyXup1
Loading provided documents...
Setting up RaptorRetriever
Setting up RetrieverQueryEngine
Mixture of Experts (MoE) is a neural network architecture that increases model capacity without proportionally increasing computation. It activates only a subset of "expert" sub-networks for each input, making it efficient for large-scale models. The structure involves input, a router, selection of top-k experts, expert processing, and a weighted output.

____________________
INFO:     127.0.0.1:55342 - "POST /chat_with_file HTTP/1.1" 200 OK
INFO:     127.0.0.1:39044 - "OPTIONS /chat HTTP/1.1" 200 OK
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC remote call 1 is done.
INFO:llama_index.core.query_engine.router_query_engine:Selecting query engine 2: The user is asking for the definition of the word "Dog", which falls under the category of definition questions as described in choice 3..
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC remote call 1 is done.
INFO:freedictionaryapi.clients.sync_client:Client has been init-ed.
INFO:freedictionaryapi.clients.base_sync_client:Send request to API with word 'Dog' and language code <LanguageCodes.ENGLISH_US: 'en_US'>. URL: 'https://api.dictionaryapi.dev/api/v2/entries/en_US/Dog'.
INFO:httpx:HTTP Request: GET https://api.dictionaryapi.dev/api/v2/entries/en_US/Dog "HTTP/1.1 200 OK"
INFO:freedictionaryapi.clients.base_client_interface:Response is successful [code=200] from url: https://api.dictionaryapi.dev/api/v2/entries/en_US/Dog.
INFO:freedictionaryapi.clients.sync_client:Client has been closed.
INFO:google_genai.models:AFC is enabled with max remote calls: 10.
INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent "HTTP/1.1 200 OK"
INFO:google_genai.models:AFC remote call 1 is done.
Generated query: ["Dog"]

Word list: ['Dog']
Result: Word: dog | Definitions: A mammal, Canis familiaris or Canis lupus familiaris, that has been domesticated for thousands of years, of highly variable appearance due to human breeding.; Any member of the Family Canidae, including domestic dogs, wolves, coyotes, jackals, foxes, and their relatives (extant and extinct); canid.; A male dog, wolf or fox, as opposed to a bitch or vixen.; A dull, unattractive girl or woman.; A man (derived from definition 2).; A coward.; Someone who is morally reprehensible.; A sexually aggressive man (cf. horny).; Any of various mechanical devices for holding, gripping, or fastening something, particularly with a tooth-like projection.; A click or pallet adapted to engage the teeth of a ratchet-wheel, to restrain the back action; a click or pawl. (See also: ratchet, windlass); A metal support for logs in a fireplace.; The eighteenth Lenormand card.; A hot dog.; Underdog.; (almost always in the plural) Foot.; (from "dog and bone") Phone or mobile phone.; One of the cones used to divide up a racetrack when training horses.; A flop; a film that performs poorly at the box office.; To pursue with the intent to catch.; To follow in an annoying or harassing way.; To fasten a hatch securely.; To watch, or participate, in sexual activity in a public place.; To intentionally restrict one's productivity as employee; to work at the slowest rate that goes unpunished.; To criticize.; To divide (a watch) with a comrade.; Meat from a dog eaten as food.; Meat prepared to be given to a dog as food.; An insult intended to assert hyperbolically that another person has value only as a corpse to be fed to a dog. | Synonyms: None | Examples: The dog barked all night long.; She’s a real dog.; He's a silly dog.; Come back and fight, you dogs!; You dirty dog.; The dogs were too hot to touch.; My dog is dead.; The woman cursed him so that trouble would dog his every step.; It is very important to dog down these hatches...; I admit that I like to dog at my local country park.; A surprise inspection of the night shift found that some workers were dogging it.; We visited South Korea this time around, where we ate dog meat for the first time.; Did you just step on my blue suede shoes? You're dog meat now!

DICTIONARY INTENT
Word: dog | Definitions: A mammal, Canis familiaris or Canis lupus familiaris, that has been domesticated for thousands of years, of highly variable appearance due to human breeding.; Any member of the Family Canidae, including domestic dogs, wolves, coyotes, jackals, foxes, and their relatives (extant and extinct); canid.; A male dog, wolf or fox, as opposed to a bitch or vixen.; A dull, unattractive girl or woman.; A man (derived from definition 2).; A coward.; Someone who is morally reprehensible.; A sexually aggressive man (cf. horny).; Any of various mechanical devices for holding, gripping, or fastening something, particularly with a tooth-like projection.; A click or pallet adapted to engage the teeth of a ratchet-wheel, to restrain the back action; a click or pawl. (See also: ratchet, windlass); A metal support for logs in a fireplace.; The eighteenth Lenormand card.; A hot dog.; Underdog.; (almost always in the plural) Foot.; (from "dog and bone") Phone or mobile phone.; One of the cones used to divide up a racetrack when training horses.; A flop; a film that performs poorly at the box office.; To pursue with the intent to catch.; To follow in an annoying or harassing way.; To fasten a hatch securely.; To watch, or participate, in sexual activity in a public place.; To intentionally restrict one's productivity as employee; to work at the slowest rate that goes unpunished.; To criticize.; To divide (a watch) with a comrade.; Meat from a dog eaten as food.; Meat prepared to be given to a dog as food.; An insult intended to assert hyperbolically that another person has value only as a corpse to be fed to a dog. | Synonyms: None | Examples: The dog barked all night long.; She’s a real dog.; He's a silly dog.; Come back and fight, you dogs!; You dirty dog.; The dogs were too hot to touch.; My dog is dead.; The woman cursed him so that trouble would dog his every step.; It is very important to dog down these hatches...; I admit that I like to dog at my local country park.; A surprise inspection of the night shift found that some workers were dogging it.; We visited South Korea this time around, where we ate dog meat for the first time.; Did you just step on my blue suede shoes? You're dog meat now!

INFO:     127.0.0.1:39044 - "POST /chat HTTP/1.1" 200 OK
[Tue May 13 23:47:22 +07 2025] Starting Uvicorn...
INFO:     Will watch for changes in these directories: ['/home/ptthanh/cvdhd/HCAI/HCAI_Edu']
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [8590] using WatchFiles
INFO:httpx:HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash "HTTP/1.1 200 OK"
INFO:     Started server process [8599]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
[Tue May 13 23:47:41 +07 2025] Script interrupted. Shutting down.
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [8599]
Tables in the database: dict_keys(['PJS_points', 'users'])
INFO:     Stopping reloader process [8590]
[Tue May 13 23:47:43 +07 2025] Uvicorn stopped.
